{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.9 标准化，让运营数据落入相同的区间\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.loadtxt('/Users/yezhibin/Downloads/书/数据分析书/python_book/chapter3/data6.txt',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-5-1e6ce4530c48>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-1e6ce4530c48>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    zscore-scaler=preprocessing.StandardScaler()\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "#z-score 标准化\n",
    "zscore-scaler=preprocessing.StandardScaler() \n",
    "data_scale_1=zscore_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.10 离散化，对连续运营数据做逻辑分层\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  amount  income             datetime    age\n",
      "0  15093    1390   10.40  2017-04-30 19:24:13   0-10\n",
      "1  15062    4024    4.68  2017-04-27 22:44:59  70-80\n",
      "2  15028    6359    3.84  2017-04-27 10:07:55  40-50\n",
      "3  15012    7759    3.70  2017-04-04 07:28:18  30-40\n",
      "4  15021     331    4.25  2017-04-08 11:14:00  70-80\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "df=pd.read_table('/Users/yezhibin/Downloads/书/数据分析书/python_book/chapter3/data7.txt',names=['id','amount','income','datetime','age']) #读取数据文件\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  amount  income datetime    age\n",
      "0  15093    1390   10.40        6   0-10\n",
      "1  15062    4024    4.68        3  70-80\n",
      "2  15028    6359    3.84        3  40-50\n",
      "3  15012    7759    3.70        1  30-40\n",
      "4  15021     331    4.25        5  70-80\n"
     ]
    }
   ],
   "source": [
    "#针对时间数据的离散化\n",
    "for i, signle_data in enumerate(df['datetime']):\n",
    "    single_data_tmp=pd.to_datetime(signle_data)\n",
    "    df['datetime'][i]=single_data_tmp.weekday()\n",
    "    \n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  amount  income datetime  age2\n",
      "0  15093    1390   10.40        6  0-40\n",
      "1  15064    7952    4.40        0  0-40\n",
      "2  15080     503    5.72        5  0-40\n",
      "3  15068    1668    3.19        5  0-40\n",
      "4  15019    6710    3.20        0  0-40\n"
     ]
    }
   ],
   "source": [
    "#针对多值离散数据的离散化\n",
    "map_df = pd.DataFrame([['0-10', '0-40'], ['10-20', '0-40'], ['20-30', '0-40'], ['30-40', '0-40'], ['40-50', '40-80'],\n",
    "                       ['50-60', '40-80'], ['60-70', '40-80'], ['70-80', '40-80'], ['80-90', '>80'], ['>90', '>80']],\n",
    "                      columns=['age', 'age2'])\n",
    "df_tmp=df.merge(map_df,left_on='age',right_on='age',how='inner')\n",
    "df=df_tmp.drop('age',1)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  amount  income datetime  age2        amount1\n",
      "0  15093    1390   10.40        6  0-40   (1000, 5000]\n",
      "1  15064    7952    4.40        0  0-40  (5000, 10000]\n",
      "2  15080     503    5.72        5  0-40    (200, 1000]\n",
      "3  15068    1668    3.19        5  0-40   (1000, 5000]\n",
      "4  15019    6710    3.20        0  0-40  (5000, 10000]\n"
     ]
    }
   ],
   "source": [
    "#针对连续数据的离散化\n",
    "#方法1：自定义分箱区间实现离散化\n",
    "bins=[0,200,1000,5000,10000] #自定义区间边界\n",
    "df['amount1']=pd.cut(df['amount'],bins)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  amount  income datetime  age2        amount1  amount2\n",
      "0  15093    1390   10.40        6  0-40   (1000, 5000]        2\n",
      "1  15064    7952    4.40        0  0-40  (5000, 10000]        1\n",
      "2  15080     503    5.72        5  0-40    (200, 1000]        2\n",
      "3  15068    1668    3.19        5  0-40   (1000, 5000]        2\n",
      "4  15019    6710    3.20        0  0-40  (5000, 10000]        1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#方法2 使用聚类法实现离散化\n",
    "data=df['amount']\n",
    "data_reshape=data.reshape((data.shape[0],1))\n",
    "model_kmeans=KMeans(n_clusters=4,random_state=0)\n",
    "keames_result=model_kmeans.fit_predict(data_reshape)\n",
    "df['amount2']=keames_result\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  income datetime  age2        amount1  amount2  amount3\n",
      "0  15093   10.40        6  0-40   (1000, 5000]        2      bad\n",
      "1  15064    4.40        0  0-40  (5000, 10000]        1  awesome\n",
      "2  15080    5.72        5  0-40    (200, 1000]        2      bad\n",
      "3  15068    3.19        5  0-40   (1000, 5000]        2      bad\n",
      "4  15019    3.20        0  0-40  (5000, 10000]        1  awesome\n"
     ]
    }
   ],
   "source": [
    "#方法3：使用4分位数实现离散化\n",
    "df['amount3']=pd.qcut(df['amount'],4,labels=['bad','medium','good','awesome'])\n",
    "df=df.drop('amount',1)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-185bba43f967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m  \u001b[0;31m# 结巴分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m  \u001b[0;31m# 基于TF-IDF的词频转向量库\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba  # 结巴分词\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # 基于TF-IDF的词频转向量库\n",
    "\n",
    "\n",
    "# 分词函数\n",
    "def jieba_cut(string):\n",
    "    word_list = []  # 建立空列表用于存储分词结果\n",
    "    seg_list = jieba.cut(string)  # 精确模式分词\n",
    "    for word in seg_list:  # 循环读取每个分词\n",
    "        word_list.append(word)  # 分词追加到列表\n",
    "    return word_list\n",
    "\n",
    "\n",
    "# 读取自然语言文件\n",
    "fn = open('/Users/yezhibin/Downloads/书/数据分析书/python_book/chapter3/text.txt')\n",
    "string_lines = fn.readlines()\n",
    "fn.close()\n",
    "\n",
    "# 中文分词\n",
    "seg_list = []  # 建立空列表，用于存储所有分词结果\n",
    "for string_line in string_lines:  # 读取每行数据\n",
    "    each_list = jieba_cut(string_line)  # 返回每行的分词结果\n",
    "    seg_list.append(each_list)  # 分词结果添加到结果列表\n",
    "for i in range(5):  # 打印输出第一行的前5条数据\n",
    "    print (seg_list[1][i])\n",
    "\n",
    "# word to vector\n",
    "stop_words = [u'\\n', u'/', u'“', u'”', u'的', u'，', u'和', u'是', u'随着', u'对于', u'对', u'等', u'能', u'都', u'。', u'、',\n",
    "              u'中', u'与', u'在', u'其']  # 自定义要去除的无用词\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, tokenizer=jieba_cut)  # 创建词向量模型\n",
    "X = vectorizer.fit_transform(string_lines)  # 将文本数据转换为向量空间模型\n",
    "vector = vectorizer.get_feature_names()  # 获得词向量\n",
    "vector_value = X.toarray()  # 获得词向量值\n",
    "vector_pd = pd.DataFrame(vector_value, columns=vector)  # 创建用于展示的数据框\n",
    "print (vector_pd.head(1))  # 打印输出第一条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-288ba6d556ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m  \u001b[0;31m# 结巴分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m  \u001b[0;31m# 基于TF-IDF的词频转向量库\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba  # 结巴分词\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # 基于TF-IDF的词频转向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
